{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"7-NLP with RNNs.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"vNkFCYXt4Iw6"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","\n","# Common imports\n","import numpy as np\n","import random\n","import os\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LgvWyfm94IxA"},"source":["# Etude 1 - Reber Grammar Verifier"]},{"cell_type":"markdown","metadata":{"id":"c8QpcQ724IxB"},"source":["Embedded Reber grammars were used by Hochreiter and Schmidhuber in their paper about LSTMs. They are artificial grammars that produce strings such as “BPBTSXXVPSEPE.” Check out <a href=\"https://www.willamette.edu/~gorr/classes/cs449/reber.html\">Jenny Orr’s</a> nice introduction to this topic. Choose a particular embedded Reber grammar (such as the one represented on Jenny Orr’s page), then train an RNN to identify whether a string respects that grammar or not. You will first need to write a function capable of generating a training batch containing about 50% strings that respect the grammar, and 50% that don’t."]},{"cell_type":"markdown","metadata":{"id":"4vGMAZgH4IxC"},"source":["Let's implement the following grammar:"]},{"cell_type":"markdown","metadata":{"id":"_8xlbnnxUF64"},"source":["![reber.gif](data:image/gif;base64,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)"]},{"cell_type":"code","metadata":{"id":"EFrX4C_F4IxD"},"source":["def generate_grammar():\n","    \"\"\"Randomly generates 'true' and non-Reber ('false') grammar strings\n","    and returns them with corresponding labels.\"\"\"\n","    \n","    reber_type = random.choice([0, 1]) # 0 - \"false\" grammar, 1 - \"true\" grammar\n","    if reber_type == 1:\n","        # true rebber grammar\n","        \n","        # the transition graph where each row corresponds to the node\n","        # and which describes the connections of this node with other nodes\n","        grammar_graph = [\n","            [('T', 1), ('P', 2)], #0\n","            [('S', 1), ('X', 3)], #1 - first upper node\n","            [('T', 2), ('V', 4)], #2 - first lower node\n","            [('X', 2), ('S', 5)], #3 - second upper node\n","            [('P', 3), ('V', 5)], #4 - second lower node\n","            [('E', -1)] #5 \n","        ]\n","    \n","    else:\n","        # non-rebber grammar\n","        grammar_graph = [\n","            [('T', 1), ('P', 2)], #0\n","            [('S', 1), ('X', 3)], #1\n","            [('X', 2), ('V', 4)], #2\n","            [('T', 2), ('P', 5)], #3\n","            [('P', 3), ('S', 5)], #4\n","            [('E', -1)] #5 \n","        ]\n","    \n","    node_idx = 0\n","    letters = ['B']\n","    \n","    # starting from row 0 of the graph, we randomly select\n","    # the transition, save the letter, and then go to the row that corresponds\n","    # to the index specified in that transition\n","    while node_idx != -1:\n","        letter, node_idx = random.choice(grammar_graph[node_idx])\n","        letters.append(letter)\n","    return letters, reber_type"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZD8Tym1f4IxH","executionInfo":{"status":"ok","timestamp":1600806529695,"user_tz":-180,"elapsed":3256,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"917e1fd9-5475-4768-b875-ae1e722ff264","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["letters, string_class = generate_grammar()\n","letters, string_class"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(['B', 'P', 'X', 'V', 'P', 'P', 'E'], 0)"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"SbyLeU2d4IxK"},"source":["First let's try to create the dataset using ragged tensors."]},{"cell_type":"code","metadata":{"id":"HwiFW7ep4IxL"},"source":["NUMBER_OF_SAMPLES = 30000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ndv2ZRLV4IxO"},"source":["## Using TextVectorization with Ragged Tensors"]},{"cell_type":"code","metadata":{"id":"TtRCREhy4IxP"},"source":["def generate_ragged_dataset(number_of_samples):\n","    \"\"\"Generates the ragged tensor as strings and normal tensor as y;\n","    also retuns the maximum string lenght in the generated dataset\n","    \"\"\"\n","    X_tensors_list = []\n","    y_tensors_list = []\n","    \n","    max_string_len = 0\n","    for sample in range(number_of_samples):\n","        letters, string_class = generate_grammar()\n","        # X_tensor = tf.ragged.constant(letters, inner_shape = (len(letters), 1), dtype='string')\n","        X_tensor = tf.ragged.constant(letters, inner_shape = (len(letters),), dtype='string')\n","        if len(letters) > max_string_len: # returns the length of the longest string\n","             max_string_len = len(letters)\n","        y_tensor = tf.constant(string_class, dtype='int64', shape=(1,))\n","        X_tensors_list.append(X_tensor)\n","        y_tensors_list.append(y_tensor)\n","    X_ragged = tf.ragged.stack([tensor for tensor in X_tensors_list], axis=0)\n","    y = tf.stack([tensor for tensor in y_tensors_list], axis=0)\n","    return X_ragged, y, max_string_len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qnQnJObj4IxS"},"source":["X, y, max_str_len = generate_ragged_dataset(NUMBER_OF_SAMPLES)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6RtQJtyJ4IxW","executionInfo":{"status":"ok","timestamp":1600806536689,"user_tz":-180,"elapsed":10001,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"0619493e-d7dd-40ea-8d83-c6ba189bc2c0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["print (f'X shape: {X.shape}, y shape: {y.shape}, max string length: {max_str_len}')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["X shape: (30000, None), y shape: (30000, 1), max string length: 38\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5XQj652g4Ixa"},"source":["dataset = tf.data.Dataset.from_tensor_slices((X, y))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qxdj6zFK4Ixd","executionInfo":{"status":"ok","timestamp":1600806536692,"user_tz":-180,"elapsed":9938,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"8e1f55d2-1d99-4c13-81ef-200745dfeb8d","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["next(iter(dataset))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(8,), dtype=string, numpy=array([b'B', b'P', b'X', b'X', b'V', b'P', b'P', b'E'], dtype=object)>,\n"," <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"mDP7gZpe4Ixg"},"source":["Let's convert each set of string characters to the string where space is used as a separator."]},{"cell_type":"code","metadata":{"id":"gEdhSjL44Ixg"},"source":["dataset = dataset.map(lambda x, y: (tf.strings.reduce_join([x], separator=' '), y))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mERwz4wOc2Z4","executionInfo":{"status":"ok","timestamp":1600806536695,"user_tz":-180,"elapsed":9887,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"61d71232-45bf-45d0-82cc-8c6e7265634f","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["next(iter(dataset))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(), dtype=string, numpy=b'B P X X V P P E'>,\n"," <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>)"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"KFSnax7B4Ixo"},"source":["First let's try to use the preprocessing.TextVectorization layer introduced in TF 2.1. To do this, we first must create a dataset consisting only of strigns and of the shape (num_samples, 1, str_len). We must also specify the number of unique letters for the layer and run the adapt() method."]},{"cell_type":"code","metadata":{"id":"PtL4vdLm4Ixo","executionInfo":{"status":"ok","timestamp":1600806536696,"user_tz":-180,"elapsed":9857,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"0cf4b95e-bf11-4f87-cd0a-aba0661c8cfd","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# here we prepare the dataset for the adapt() method by adding an extra dimension\n","text_dataset = dataset.map(lambda x, y: tf.expand_dims(x, 0))\n","for item in text_dataset.take(2):\n","    print(item)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tf.Tensor([b'B P X X V P P E'], shape=(1,), dtype=string)\n","tf.Tensor([b'B T S S X X V V E'], shape=(1,), dtype=string)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"j3Dzf-3f4Ixs"},"source":["Now let's run the adapt() method."]},{"cell_type":"code","metadata":{"id":"Tj-pFI1B4Ixs"},"source":["NUM_LETTERS = 7 \n","\n","vectorize_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n","  # 7 + 1 for oov bucket, supposely (not sure)\n","  # if specifying 7 as max_token, for some reason\n","  # we get the vocabulary stripped by one symbol 'b' (i.e. of lenght 6, not 7)\n","  max_tokens=NUM_LETTERS + 2,\n","  output_mode='int',\n","  output_sequence_length=max_str_len)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BTgzGVQe4Ixw"},"source":["try:\n","    vectorize_layer.adapt(text_dataset)\n","except tf.errors.InvalidArgumentError as exc:\n","    print(exc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"alLFNZyG4Ixz","executionInfo":{"status":"ok","timestamp":1600806559501,"user_tz":-180,"elapsed":32589,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"836ee7e1-3acc-4aee-f086-75cc7c2433ba","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["vectorize_layer.get_vocabulary()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['', '[UNK]', 't', 'v', 'p', 'x', 'e', 'b', 's']"]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"q8ofXPt94Ix2"},"source":["model = keras.models.Sequential([\n","    keras.layers.Input(shape=(1,), dtype=tf.string),\n","    vectorize_layer,\n","    # without the mask_zero parameter, the training takes much longer;\n","    # probably, ragged tensors are masked with zeros too\n","    # unfortunately, the usage of the mask_zero parameter with the vectorize layer is not documented well\n","    keras.layers.Embedding(NUM_LETTERS + 2, 40, mask_zero=True),\n","    keras.layers.GRU(100),\n","    keras.layers.Dense(1, activation='sigmoid')\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HQvjScqp4Ix6"},"source":["train_size = int(0.7 * NUMBER_OF_SAMPLES)\n","val_size = int(0.15 * NUMBER_OF_SAMPLES)\n","test_size = int(0.15 * NUMBER_OF_SAMPLES)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1HxhL47Z4Ix9"},"source":["train = dataset.take(train_size)\n","test = dataset.skip(train_size)\n","val = test.take(val_size)\n","test = test.skip(test_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ntoG8qhg4IyA","executionInfo":{"status":"ok","timestamp":1600806560532,"user_tz":-180,"elapsed":33492,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"2ded54d3-7747-41a5-993a-ab1e2eab9f93","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["for item in train.take(2):\n","    print(item)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(<tf.Tensor: shape=(), dtype=string, numpy=b'B P X X V P P E'>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([0])>)\n","(<tf.Tensor: shape=(), dtype=string, numpy=b'B T S S X X V V E'>, <tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"vH4g4Yam4IyF"},"source":["train_ds = train.shuffle(100).repeat().batch(32)\n","test_ds = test.batch(32)  \n","valid_ds = val.batch(32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Uu0U_rb4IyI"},"source":["model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nEBDwXDG4IyL","executionInfo":{"status":"ok","timestamp":1600806610683,"user_tz":-180,"elapsed":83575,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"8b7cdd3e-c377-40e6-900a-013da6b097c1","colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["history = model.fit(train_ds, epochs=5, validation_data=valid_ds,\n","                    steps_per_epoch=int(train_size / 32), validation_steps=int(val_size / 32))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","656/656 [==============================] - 10s 15ms/step - loss: 0.1249 - accuracy: 0.9322 - val_loss: 2.1650e-04 - val_accuracy: 1.0000\n","Epoch 2/5\n","656/656 [==============================] - 9s 13ms/step - loss: 1.0727e-04 - accuracy: 1.0000 - val_loss: 5.6156e-05 - val_accuracy: 1.0000\n","Epoch 3/5\n","656/656 [==============================] - 9s 13ms/step - loss: 3.6699e-05 - accuracy: 1.0000 - val_loss: 2.4864e-05 - val_accuracy: 1.0000\n","Epoch 4/5\n","656/656 [==============================] - 9s 13ms/step - loss: 1.7983e-05 - accuracy: 1.0000 - val_loss: 1.3397e-05 - val_accuracy: 1.0000\n","Epoch 5/5\n","656/656 [==============================] - 9s 13ms/step - loss: 1.0205e-05 - accuracy: 1.0000 - val_loss: 7.9812e-06 - val_accuracy: 1.0000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"jgCJRtpF4IyP","executionInfo":{"status":"ok","timestamp":1600806611768,"user_tz":-180,"elapsed":84620,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"66d97370-51b6-4521-a16b-872ed36eb2c1","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["model.evaluate(test_ds, steps=int(test_size / 32))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["140/140 [==============================] - 1s 6ms/step - loss: 7.9203e-06 - accuracy: 1.0000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[7.920263669802807e-06, 1.0]"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"pYSP5_TIq41m"},"source":["We achieved 100% accuracy."]},{"cell_type":"markdown","metadata":{"id":"Vvp_AN9y4IyU"},"source":["## Using Keras.preprocessing.text.Tokenizer"]},{"cell_type":"markdown","metadata":{"id":"rJTRygPj4IyV"},"source":["Let's re-write our dataset generation function a bit so that we can use the Tokenizer. "]},{"cell_type":"code","metadata":{"id":"UimJy8MT4IyW"},"source":["def generate_dataset(number_of_samples):\n","    \"\"\"Generates the list of strings and the list of classes.\"\"\"\n","    X_list = []\n","    y_list = []\n","    \n","    for sample in range(number_of_samples):\n","        letters, string_class = generate_grammar()\n","        X_list.append(letters)\n","        y_list.append(string_class)\n","   \n","    return X_list, y_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a725tR-i4Iya"},"source":["X, y = generate_dataset(NUMBER_OF_SAMPLES)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9VmnqzBD4Iyd"},"source":["def sequence_strings(strings):\n","    \"\"\"Converts the list of strings to corresponding sequences\"\"\"\n","    tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n","    tokenizer.fit_on_texts(strings)\n","    \n","    # convert strings to indices\n","    tokenized_X = tokenizer.texts_to_sequences(strings)\n","    \n","    # pads to the longest sequence\n","    sequenced_X = tf.keras.preprocessing.sequence.pad_sequences(tokenized_X, padding='post')\n","    return sequenced_X, tokenizer # can be used for converting new strings "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PjYDGL064Iyf"},"source":["X, tokenizer = sequence_strings(X)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8hnhdW6d4Iyi"},"source":["dataset = tf.data.Dataset.from_tensor_slices((X, y))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ij4JKyGI4Iyl","executionInfo":{"status":"ok","timestamp":1600806612580,"user_tz":-180,"elapsed":85278,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"eb2151cc-d1b3-4162-cd46-49e429032654","colab":{"base_uri":"https://localhost:8080/","height":85}},"source":["next(iter(dataset))"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<tf.Tensor: shape=(38,), dtype=int32, numpy=\n"," array([6, 3, 5, 5, 2, 3, 4, 1, 3, 2, 2, 4, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>,\n"," <tf.Tensor: shape=(), dtype=int32, numpy=0>)"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"ZtA921lJ4Iyo"},"source":["model_token = keras.models.Sequential([\n","    keras.layers.Embedding(len(tokenizer.word_counts)+1, 40, mask_zero=True),\n","    keras.layers.GRU(100),\n","    keras.layers.Dense(1, activation='sigmoid')\n","])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aC05DjFI4Iyr"},"source":["train = dataset.take(train_size)\n","test = dataset.skip(train_size)\n","val = test.take(val_size)\n","test = test.skip(test_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"llmvtSnD4Iyt","executionInfo":{"status":"ok","timestamp":1600806613545,"user_tz":-180,"elapsed":86122,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"6395ff25-7086-47a3-d7a9-6cbfe51752aa","colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["for item in train.take(2):\n","    print(item)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["(<tf.Tensor: shape=(38,), dtype=int32, numpy=\n","array([6, 3, 5, 5, 2, 3, 4, 1, 3, 2, 2, 4, 5, 7, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n","(<tf.Tensor: shape=(38,), dtype=int32, numpy=\n","array([6, 3, 5, 2, 1, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int32)>, <tf.Tensor: shape=(), dtype=int32, numpy=0>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SoZzae-f4Iyv"},"source":["train_ds = train.shuffle(100).repeat().batch(32)\n","test_ds = test.batch(32)  \n","valid_ds = val.batch(32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CRvviGJ24Iyy"},"source":["model_token.compile(loss=\"binary_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9r08p3O84Iy0","executionInfo":{"status":"ok","timestamp":1600806650026,"user_tz":-180,"elapsed":122435,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"aa1aa76e-46cd-4c2c-8d44-f46e8d343a32","colab":{"base_uri":"https://localhost:8080/","height":187}},"source":["history = model_token.fit(train_ds, epochs=5, validation_data=valid_ds,\n","                    steps_per_epoch=int(train_size / 32), validation_steps=int(val_size / 32))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","656/656 [==============================] - 7s 11ms/step - loss: 0.1383 - accuracy: 0.9254 - val_loss: 2.5459e-04 - val_accuracy: 1.0000\n","Epoch 2/5\n","656/656 [==============================] - 6s 10ms/step - loss: 1.2401e-04 - accuracy: 1.0000 - val_loss: 5.8348e-05 - val_accuracy: 1.0000\n","Epoch 3/5\n","656/656 [==============================] - 6s 10ms/step - loss: 3.8772e-05 - accuracy: 1.0000 - val_loss: 2.4439e-05 - val_accuracy: 1.0000\n","Epoch 4/5\n","656/656 [==============================] - 6s 9ms/step - loss: 1.8143e-05 - accuracy: 1.0000 - val_loss: 1.2719e-05 - val_accuracy: 1.0000\n","Epoch 5/5\n","656/656 [==============================] - 6s 10ms/step - loss: 9.9909e-06 - accuracy: 1.0000 - val_loss: 7.3823e-06 - val_accuracy: 1.0000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_XtXjFge4Iy3","executionInfo":{"status":"ok","timestamp":1600806650400,"user_tz":-180,"elapsed":122763,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"a8f49d16-2d39-4f46-ef73-448f58dc381c","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["model_token.evaluate(test_ds, steps=int(test_size / 32))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["140/140 [==============================] - 1s 4ms/step - loss: 7.5801e-06 - accuracy: 1.0000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[7.580071724078152e-06, 1.0]"]},"metadata":{"tags":[]},"execution_count":36}]},{"cell_type":"markdown","metadata":{"id":"lIV-gSKy4Iy5"},"source":["# Etude 2 - Encoder-Decoder for Converting Date String Formats"]},{"cell_type":"markdown","metadata":{"id":"f8uIADar4Iy6"},"source":["Train an Encoder–Decoder model that can convert a date string from one format to another (e.g., from “April 22, 2019” to “2019-04-22”)."]},{"cell_type":"code","metadata":{"id":"f5zTG6obN_M6"},"source":["MAX_SEQUENCE_LENGTH = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PUQ8RXUi4Iy6"},"source":["from datetime import date\n","import random\n","\n","def generate_date(number_of_samples):\n","    \"\"\"Generates the same date in various formats and returns two lists - with generated dates and target dates\"\"\"\n","    start_date = date.today().replace(day=1, month=1, year=1980).toordinal()\n","    end_date = date.today().toordinal()\n","    \n","    X_list = []\n","    y_list = []\n","    \n","    for i in range(number_of_samples):\n","        random_day = date.fromordinal(random.randint(start_date, end_date))\n","        # 'July 09, 1985', '1985-July-09', '09/07/1985'\n","        sample_formats = [\"%B %d, %Y\", \"%Y-%B-%d\", \"%d/%m/%Y\"]\n","        X = random_day.strftime(random.choice(sample_formats))\n","        y = random_day.strftime(\"%Y-%m-%d\") # '1985-07-09'\n","        X_list.append(X)\n","        y_list.append(y)\n","    return X_list, y_list        "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dM02MBP_4Iy9"},"source":["X_dates, y_dates = generate_date(NUMBER_OF_SAMPLES)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"stB1mxdD4Iy_"},"source":["Let's convert each date to a sequence."]},{"cell_type":"code","metadata":{"id":"teu98y_N4IzA"},"source":["X, tokenizer_X = sequence_strings(X_dates)\n","y, tokenizer_y = sequence_strings(y_dates)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S8ObaNIm4IzH"},"source":["X_vocab_length = len(tokenizer_X.index_word)\n","y_vocab_length = len(tokenizer_y.index_word)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vmh5DN5T4IzK"},"source":["# converts list of lists into an array\n","X = np.asarray(X, dtype=np.int32)\n","y = np.asarray(y, dtype=np.int32)\n","X_decoder = np.c_[np.zeros((X.shape[0], 1)), y[:, :-1]] # y that starts with 0 and has no last character"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N6jNVwBmLeiy"},"source":["Note that since the tokenizer starts word index with 1, we will use 0 as our \"sos\" word. In `X_decoder`, our sos word is represented with np.zeros((X.shape[0])."]},{"cell_type":"code","metadata":{"id":"-r7PLVEm4IzP"},"source":["from sklearn.model_selection import train_test_split\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","X_decoder, _, _, _ = train_test_split(X_decoder, y, test_size=0.2, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DAEdhnFyCjCI","executionInfo":{"status":"ok","timestamp":1600806651769,"user_tz":-180,"elapsed":123866,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"f3abe358-7ebe-4cf2-f7c0-83cac3b25e5b","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y_train[0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([4, 1, 1, 8, 2, 1, 5, 2, 1, 3], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":44}]},{"cell_type":"code","metadata":{"id":"a6vP-a_-Clv7","executionInfo":{"status":"ok","timestamp":1600806651770,"user_tz":-180,"elapsed":123820,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"e3d87f38-2199-49b8-f7da-8bcb42593f12","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y_train[1]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 4,  1,  3,  4,  2,  3,  4,  2,  4, 11], dtype=int32)"]},"metadata":{"tags":[]},"execution_count":45}]},{"cell_type":"markdown","metadata":{"id":"YixYL-LX4IzX"},"source":["Let's adapt one of simpler implementations of the encoder-decoder model."]},{"cell_type":"code","metadata":{"id":"1KexONy3Cvsv","executionInfo":{"status":"ok","timestamp":1600806651770,"user_tz":-180,"elapsed":123783,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"600a2d86-b950-47e7-e740-c9794f2f5939","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["y_train.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(24000, 10)"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"KiwYkKon4IzX"},"source":["import tensorflow_addons as tfa\n","\n","embed_size = 10\n","\n","encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n","decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n","\n","embeddings_enc = keras.layers.Embedding(X_vocab_length+1, embed_size) # +1 for oov\n","embeddings_dec = keras.layers.Embedding(y_vocab_length+1, embed_size)\n","\n","encoder_embeddings = embeddings_enc(encoder_inputs)\n","decoder_embeddings = embeddings_dec(decoder_inputs)\n","\n","encoder = keras.layers.LSTM(512, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n","encoder_state = [state_h, state_c]\n","\n","sampler = tfa.seq2seq.sampler.TrainingSampler()\n","\n","decoder_cell = keras.layers.LSTMCell(512)\n","output_layer = keras.layers.Dense(y_vocab_length+1)\n","decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell, sampler,\n","                                                 output_layer=output_layer)\n","final_outputs, final_state, final_sequence_lengths = decoder(\n","    decoder_embeddings, initial_state=encoder_state)\n","Y_proba = tf.nn.softmax(final_outputs.rnn_output)\n","\n","model_enc_dec = keras.models.Model(\n","    inputs=[encoder_inputs, decoder_inputs],\n","    outputs=[Y_proba])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OrgMB9O54Izd"},"source":["model_enc_dec.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aP5cWxNx4Izf","executionInfo":{"status":"ok","timestamp":1600807185898,"user_tz":-180,"elapsed":657819,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"7d26cd2d-77bc-48f4-8ada-ce07c7a5d466","colab":{"base_uri":"https://localhost:8080/","height":867}},"source":["history = model_enc_dec.fit([X_train, X_decoder], y_train, epochs=25)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/25\n","750/750 [==============================] - 21s 28ms/step - loss: 1.1047 - accuracy: 0.5224\n","Epoch 2/25\n","750/750 [==============================] - 21s 28ms/step - loss: 0.9391 - accuracy: 0.5664\n","Epoch 3/25\n","750/750 [==============================] - 21s 28ms/step - loss: 0.8208 - accuracy: 0.6140\n","Epoch 4/25\n","750/750 [==============================] - 21s 28ms/step - loss: 0.7451 - accuracy: 0.6573\n","Epoch 5/25\n","750/750 [==============================] - 21s 28ms/step - loss: 0.6619 - accuracy: 0.7095\n","Epoch 6/25\n","750/750 [==============================] - 21s 28ms/step - loss: 0.4256 - accuracy: 0.8179\n","Epoch 7/25\n","750/750 [==============================] - 21s 28ms/step - loss: 0.1798 - accuracy: 0.9263\n","Epoch 8/25\n","750/750 [==============================] - 21s 28ms/step - loss: 0.0664 - accuracy: 0.9760\n","Epoch 9/25\n","750/750 [==============================] - 21s 28ms/step - loss: 0.0250 - accuracy: 0.9922\n","Epoch 10/25\n","750/750 [==============================] - 21s 28ms/step - loss: 0.0200 - accuracy: 0.9940\n","Epoch 11/25\n","750/750 [==============================] - 21s 28ms/step - loss: 0.0010 - accuracy: 1.0000\n","Epoch 12/25\n","750/750 [==============================] - 21s 28ms/step - loss: 4.0096e-04 - accuracy: 1.0000\n","Epoch 13/25\n","750/750 [==============================] - 21s 28ms/step - loss: 2.2223e-04 - accuracy: 1.0000\n","Epoch 14/25\n","750/750 [==============================] - 21s 28ms/step - loss: 1.3149e-04 - accuracy: 1.0000\n","Epoch 15/25\n","750/750 [==============================] - 21s 28ms/step - loss: 8.0628e-05 - accuracy: 1.0000\n","Epoch 16/25\n","750/750 [==============================] - 21s 28ms/step - loss: 5.0512e-05 - accuracy: 1.0000\n","Epoch 17/25\n","750/750 [==============================] - 21s 28ms/step - loss: 3.1823e-05 - accuracy: 1.0000\n","Epoch 18/25\n","750/750 [==============================] - 21s 28ms/step - loss: 2.0147e-05 - accuracy: 1.0000\n","Epoch 19/25\n","750/750 [==============================] - 21s 28ms/step - loss: 1.2978e-05 - accuracy: 1.0000\n","Epoch 20/25\n","750/750 [==============================] - 21s 28ms/step - loss: 8.4823e-06 - accuracy: 1.0000\n","Epoch 21/25\n","750/750 [==============================] - 21s 28ms/step - loss: 5.5919e-06 - accuracy: 1.0000\n","Epoch 22/25\n","750/750 [==============================] - 21s 28ms/step - loss: 3.6871e-06 - accuracy: 1.0000\n","Epoch 23/25\n","750/750 [==============================] - 21s 28ms/step - loss: 2.4534e-06 - accuracy: 1.0000\n","Epoch 24/25\n","750/750 [==============================] - 21s 28ms/step - loss: 1.6389e-06 - accuracy: 1.0000\n","Epoch 25/25\n","750/750 [==============================] - 21s 28ms/step - loss: 1.1054e-06 - accuracy: 1.0000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"c8go26S7tyQp"},"source":["Now let's test out model on the test data."]},{"cell_type":"code","metadata":{"id":"v6FGelVn-mkF","executionInfo":{"status":"ok","timestamp":1600807189410,"user_tz":-180,"elapsed":661259,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"19010127-45ba-429b-983e-b9461b8502ca","colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["# for the encoder to be able to predict test arrays, we\n","# we provide y_test in X_decoder for evaluation purposes;\n","# at inference, we should feed each predicted character\n","X_decoder_test = np.c_[np.zeros((X_test.shape[0], 1)), y_test[:, :-1]]\n","\n","model_enc_dec.evaluate([X_test, X_decoder_test], y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["188/188 [==============================] - 3s 15ms/step - loss: 9.7319e-07 - accuracy: 1.0000\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[9.731853651828715e-07, 1.0]"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"markdown","metadata":{"id":"3hzElsgh-uLL"},"source":["Now let's write the function that makes predictions. Аt inference time, we  should pass each predicted character to *X_decoder*."]},{"cell_type":"code","metadata":{"id":"Dq_5Rwlb6Bmg"},"source":["our_text = [\"21/12/1993\", \"december 04, 2004\", \"january 14, 1983\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kxwE7KPzQ0XK"},"source":["def convert_date(text, model):\n","    # converting strings to sequences and padding to the longest sequence\n","    tokenized_text = tokenizer_X.texts_to_sequences(text)\n","    tokenized_padded = tf.keras.preprocessing.sequence.pad_sequences(tokenized_text, padding='post')\n","    # converting to a numpy array\n","    text_array = np.asarray(tokenized_padded, dtype=np.int32)\n","    Y_pred = tf.fill(dims=(text_array.shape[0], 1), value=0)\n","    \n","    for index in range(MAX_SEQUENCE_LENGTH):\n","        pad_size = text_array.shape[1] - Y_pred.shape[1]\n","        X_decoder_pred = tf.pad(Y_pred, [[0, 0], [0, pad_size]])\n","        Y_probas_next = model_enc_dec.predict([text_array, X_decoder_pred])[:, index:index+1]\n","        Y_pred_next = tf.argmax(Y_probas_next, axis=-1, output_type=tf.int32)\n","        Y_pred = tf.concat([Y_pred, Y_pred_next], axis=1)\n","    predictions = tokenizer_y.sequences_to_texts(Y_pred[:, 1:].numpy())\n","    return predictions"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9mUaaVeKRmva","executionInfo":{"status":"ok","timestamp":1600807190650,"user_tz":-180,"elapsed":662398,"user":{"displayName":"Constantin A-Z","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhNGkIJiOXJgItKElmAs11-s5Anr8SBykphIpSmbg=s64","userId":"07698803386934159715"}},"outputId":"1618ef16-a15d-4cc7-8dfe-b506ec571d4e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["convert_date(our_text, model_enc_dec)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['1 9 9 3 - 1 2 - 2 1', '2 0 0 4 - 1 2 - 0 4', '1 9 8 3 - 0 1 - 1 4']"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"id":"BHoZzypA53MO"},"source":["We see that our prediction correspond to the desired result."]}]}