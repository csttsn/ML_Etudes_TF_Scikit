{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"colab":{"name":"3-Ensemble Learning and Random Forests.ipynb","provenance":[],"toc_visible":true}},"cells":[{"cell_type":"code","metadata":{"id":"W1a9VrTjIj01"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns; sns.set()\n","\n","from sklearn.datasets import fetch_openml \n","\n","from sklearn.svm import SVC\n","from sklearn.svm import LinearSVC\n","from sklearn.ensemble import ExtraTreesClassifier\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.model_selection import train_test_split\n","\n","np.random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DlDZk4ITIj05"},"source":["import matplotlib as mpl\n","mpl.rcParams['figure.figsize'] = (9, 6)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QvuxKKwzIj09"},"source":["# Etude 1 - Ensemble Learning"]},{"cell_type":"markdown","metadata":{"id":"jN4HxFnWIj0-"},"source":["Load the MNIST data and split it into a training set, a validation set, and a test set (e.g., use the first 50,000 instances for training, the next 10,000 for validation, and the last 10,000 for testing). Then train various classifiers, such as a Random Forest classifier, an Extra-Trees classifier, and an SVM. Next, try to combine them into an ensemble that outperforms them all on the validation set, using a soft or hard voting classifier. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?"]},{"cell_type":"code","metadata":{"id":"cGpnOKA9Ij0-"},"source":["mnist = fetch_openml('mnist_784', version=1)\n","mnist.target = mnist.target.astype(np.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YuI9_eB9Ij1C"},"source":["X, y = mnist[\"data\"], mnist[\"target\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V_wKiPwDIj1F","outputId":"ce80d2f6-3b9a-441f-cf86-0aba3ec72311"},"source":["X.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(70000, 784)"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"pZpBZDm9Ij1J"},"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=50000, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NGY1ONpFIj1M"},"source":["X_test, X_val, y_test, y_val = train_test_split(X_test, y_test, train_size=10000, random_state=42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CzIhzvwOIj1Q"},"source":["Let's train 4 classifiers: Random Forest classifier, Extra-Trees classifier, SVM, and KNN. Then let's look at their accuracy."]},{"cell_type":"code","metadata":{"id":"AqmoD5xmIj1R"},"source":["rforest = RandomForestClassifier(n_jobs=-1)\n","etree = ExtraTreesClassifier(n_jobs=-1)\n","svc = LinearSVC(max_iter=1500)\n","knn = KNeighborsClassifier(n_jobs=-1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSrgS597Ij1U","outputId":"9367eba5-3f6e-4538-8d2b-aa081eec284c"},"source":["classifiers = [rforest, etree, svc, knn]\n","for classifier in classifiers:\n","    print(classifier)\n","    classifier.fit(X_train, y_train)  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["RandomForestClassifier(n_jobs=-1)\n","ExtraTreesClassifier(n_jobs=-1)\n","LinearSVC(max_iter=1500)\n"],"name":"stdout"},{"output_type":"stream","text":["C:\\Users\\CS\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n","  \"the number of iterations.\", ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["KNeighborsClassifier(n_jobs=-1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VhHypwj9Ij1X"},"source":["Let's view the score for each particular classifier. "]},{"cell_type":"code","metadata":{"id":"Ir0cHU4dIj1Y","outputId":"146374f0-d112-4c17-a6b4-055045d524e6"},"source":["[classifier.score(X_val, y_val) for classifier in classifiers]    "],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.9672, 0.9678, 0.8612, 0.9675]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"_QKEibx7Ij1b"},"source":["Now we combine these classifiers into a VotingClassifier ensemble:"]},{"cell_type":"code","metadata":{"id":"U_pYGF9AIj1c"},"source":["vote_clf = VotingClassifier(\n","    [('rforest', rforest), ('etree', etree), ('svc', svc), ('knn', knn)],\n","    n_jobs=-1\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LxtbZkipIj1f","outputId":"17c0681f-84c3-4f68-8e7d-b7902ef5664c"},"source":["vote_clf.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VotingClassifier(estimators=[('rforest', RandomForestClassifier(n_jobs=-1)),\n","                             ('etree', ExtraTreesClassifier(n_jobs=-1)),\n","                             ('svc', LinearSVC(max_iter=1500)),\n","                             ('knn', KNeighborsClassifier(n_jobs=-1))],\n","                 n_jobs=-1)"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"MWQ5xDfDIj1i","outputId":"f5aaaf25-6137-4c5a-8330-c9c52ba75ab3"},"source":["vote_clf.score(X_val, y_val)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9683"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"WY3sqoWfIj1l"},"source":["Let's view the score for each estimator in the voting classifier:"]},{"cell_type":"code","metadata":{"id":"IMYYr4dqIj1m","outputId":"f42fc5b4-f5d8-416b-fca4-aba945569b4d"},"source":["[estimator.score(X_val, y_val) for estimator in vote_clf.estimators_]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[0.9667, 0.9699, 0.8371, 0.9675]"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"quj1-J4hIj1p"},"source":["The third classifier (SVC) produces the weakest score. Let's remove it from the voting classifier."]},{"cell_type":"code","metadata":{"id":"BFPmXclUIj1q"},"source":["del vote_clf.estimators_[2]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LI-u2EyUIj1t","outputId":"73aec135-ba83-4524-c546-5e5919498d89"},"source":["vote_clf.estimators_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[RandomForestClassifier(n_jobs=-1),\n"," ExtraTreesClassifier(n_jobs=-1),\n"," KNeighborsClassifier(n_jobs=-1)]"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"Ico-ylfrIj1w","outputId":"76a8ddd1-269d-41c6-b0d5-e55bd2030a51"},"source":["vote_clf.score(X_val, y_val)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.971"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"5TJiYyT-Ij13"},"source":["The score indeed improved a bit. Now let's use the same voting classifier but with the \"soft\" voting."]},{"cell_type":"code","metadata":{"id":"rJV4PPG8Ij14","outputId":"f37719bd-d73f-43f6-842d-d3895d06f289"},"source":["vote_clf.set_params(voting='soft')"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["VotingClassifier(estimators=[('rforest', RandomForestClassifier(n_jobs=-1)),\n","                             ('etree', ExtraTreesClassifier(n_jobs=-1)),\n","                             ('svc', LinearSVC(max_iter=1500)),\n","                             ('knn', KNeighborsClassifier(n_jobs=-1))],\n","                 n_jobs=-1, voting='soft')"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"34xSPdJdIj17"},"source":["Note: If we run the fit method without removing LinearSVC from the ensemble, we will get an error (since this classifier cannot return probabilities)."]},{"cell_type":"code","metadata":{"id":"LjZQgRz7Ij18","outputId":"009b7917-046a-4f59-db2f-d4157c78b8c7"},"source":["vote_clf.estimators_"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[RandomForestClassifier(n_jobs=-1),\n"," ExtraTreesClassifier(n_jobs=-1),\n"," KNeighborsClassifier(n_jobs=-1)]"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"markdown","metadata":{"id":"7Uq8q6sMIj1-"},"source":["Let's estimate its accuracy on the validation set."]},{"cell_type":"code","metadata":{"id":"pA-GIY4ZIj1_","outputId":"c336e662-83f0-46bd-d77b-1a2f0023115d"},"source":["vote_clf.score(X_val, y_val)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9733"]},"metadata":{"tags":[]},"execution_count":20}]},{"cell_type":"markdown","metadata":{"id":"mjXJaArnIj2B"},"source":["We obtained a slighly higher result than with soft  voting. Now let's estimate this classifier accuracy on the test set."]},{"cell_type":"code","metadata":{"id":"LuWHuv9SIj2C","outputId":"3d369d82-4f47-4e6b-ec50-03066e2b9857"},"source":["vote_clf.score(X_test, y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.9744"]},"metadata":{"tags":[]},"execution_count":21}]},{"cell_type":"markdown","metadata":{"id":"0NgQ8Je2Ij2E"},"source":["Note: you can get the same result by running predict() on the X_test and then using the accuracy_score() function."]},{"cell_type":"markdown","metadata":{"id":"baj7qVaFIj2F"},"source":["# Etude 2 - Blender"]},{"cell_type":"markdown","metadata":{"id":"i5iniYcVIj2G"},"source":["Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions:\n","each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class. Congratulations, you have just trained a blender, and together with the classifiers they form a stacking ensemble! Now let’s evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions. How does it compare to the voting classifier you trained earlier?"]},{"cell_type":"code","metadata":{"id":"uc9iSaKLIj2H"},"source":["def blender(X, classifiers):\n","    y_blend = np.empty([len(X), len(classifiers)])\n","    for index, classifier in enumerate(classifiers):\n","        y_blend[:, index] = classifier.predict(X)\n","    return y_blend"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T0NcIdc6Ij2K"},"source":["Now we train the blender using the validation set (we should use the set that was not used in any way to train any of the predictors)."]},{"cell_type":"code","metadata":{"id":"yHKzhAN5Ij2L","outputId":"2e634d5d-f75d-4cc7-d7a6-519b75f44e7d"},"source":["blender_rf = RandomForestClassifier(n_jobs=-1)\n","blender_rf.fit(blender(X_val, classifiers), y_val)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(n_jobs=-1)"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"hr56ziA6Ij2P"},"source":["Now let's test out blender on the test set."]},{"cell_type":"code","metadata":{"id":"aThPYTS0Ij2P","outputId":"86f2ade8-e9b0-4660-a727-dab000d0142d"},"source":["blender_rf.score(blender(X_test, classifiers), y_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.972"]},"metadata":{"tags":[]},"execution_count":24}]}]}